{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reverb.training.utils import DEFAULT_TRAINING_KWARGS, DEFAULT_MODEL_KWARGS, DEFAULT_DATA_KWARGS\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "multiclass_experiments = {\n",
    "    \"baseline\": {\n",
    "        \"run_name\": \"ablations/multiclass/baseline\",\n",
    "        \"training_kwargs\": {\n",
    "            \"max_epochs\": 25,\n",
    "            'class_weights': [0.5, 1.0, 1.0]\n",
    "        },\n",
    "        \"model_kwargs\": {\"classes\": 3},\n",
    "        \"data_kwargs\": {\"feature_class\": \"multiclass\"},\n",
    "    },\n",
    "\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.24s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.05s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "from reverb.training.utils import train, get_eval_dataloaders, compute_results_over_eval_sets, save_evaluation_results\n",
    "eval_dataloaders = get_eval_dataloaders(feature_class=\"multiclass\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for experiment in multiclass_experiments.keys():\n",
    "    experiment_config = multiclass_experiments[experiment]\n",
    "    for i in range(1,3):\n",
    "        run_name = f\"{experiment_config['run_name']}_{i}\"\n",
    "\n",
    "        training_kwargs = experiment_config['training_kwargs']\n",
    "        model_kwargs = experiment_config['model_kwargs']\n",
    "        data_kwargs = experiment_config['data_kwargs']\n",
    "        # train the model\n",
    "        train(\n",
    "            run_name=run_name,\n",
    "            mode=\"supervised\",\n",
    "            model_kwargs=model_kwargs,\n",
    "            data_kwargs=data_kwargs,  \n",
    "            training_kwargs=training_kwargs,\n",
    "        )\n",
    "        # Evaluate the model\n",
    "        results = compute_results_over_eval_sets(run_name, eval_dataloaders, model_kwargs=model_kwargs, training_kwargs=training_kwargs)\n",
    "        save_evaluation_results(run_name, results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved individual repeat results and summary statistics.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "experiment_names = multiclass_experiments.keys()\n",
    "\n",
    "# Root directory containing experiment folders like 'baseline_model_0/', 'baseline_model_1/', etc.\n",
    "experiments_root = './checkpoints/ablations/multiclass'\n",
    "\n",
    "flattened_data = []\n",
    "\n",
    "for exp_name in experiment_names:\n",
    "    # Find folders starting with the experiment name and ending in a number (repeats)\n",
    "    matching_folders = [\n",
    "        d for d in os.listdir(experiments_root)\n",
    "        if os.path.isdir(os.path.join(experiments_root, d)) and d.startswith(exp_name + '_')\n",
    "    ]\n",
    "\n",
    "    for folder in matching_folders:\n",
    "        results_path = os.path.join(experiments_root, folder, 'eval_results.json')\n",
    "        if os.path.isfile(results_path):\n",
    "            with open(results_path, 'r') as f:\n",
    "                datasets = json.load(f)\n",
    "            for dataset, metrics in datasets.items():\n",
    "                for metric, value in metrics.items():\n",
    "                    if metric in ['miou', 'precision', 'recall']:\n",
    "                        flattened_data.append({\n",
    "                            'Experiment': exp_name,  # Group under common experiment name\n",
    "                            'Repeat': folder,\n",
    "                            'Dataset': dataset,\n",
    "                            'Metric': metric,\n",
    "                            'Value': value\n",
    "                        })\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(flattened_data)\n",
    "\n",
    "# Compute mean and SEM over repeats for each experiment\n",
    "mean_df = (\n",
    "    df.groupby(['Experiment', 'Dataset', 'Metric'])['Value']\n",
    "    .mean()\n",
    "    .reset_index()\n",
    "    .rename(columns={'Value': 'Mean'})\n",
    ")\n",
    "\n",
    "sem_df = (\n",
    "    df.groupby(['Experiment', 'Dataset', 'Metric'])['Value']\n",
    "    .sem()\n",
    "    .reset_index()\n",
    "    .rename(columns={'Value': 'Std_Error'})\n",
    ")\n",
    "\n",
    "# Merge summaries\n",
    "summary_df = pd.merge(mean_df, sem_df, on=['Experiment', 'Dataset', 'Metric'])\n",
    "\n",
    "# Save outputs\n",
    "df.to_csv('individual_repeat_results.csv', index=False)\n",
    "summary_df.to_csv('multiclass_experiment_summary.csv', index=False)\n",
    "\n",
    "print(\"Saved individual repeat results and summary statistics.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter only for 'miou'\n",
    "miou_df = summary_df[summary_df['Metric'] == 'miou']\n",
    "\n",
    "# Print one table per dataset\n",
    "for dataset in miou_df['Dataset'].unique():\n",
    "    print(f\"\\n--- Dataset: {dataset} ---\")\n",
    "    display(miou_df[miou_df['Dataset'] == dataset].drop(columns=['Metric']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reverb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
